{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify this cell!\n",
    "\n",
    "# Import necessary libraries\n",
    "# DO NOT IMPORT OTHER LIBRARIES - This will break the autograder.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os, shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from environment import BaseEnvironment\n",
    "from agent import BaseAgent\n",
    "from optimizer import BaseOptimizer\n",
    "import plot_script\n",
    "from randomwalk_environment import RandomWalkEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_matmul(x1, x2):\n",
    "    \"\"\"\n",
    "    Given matrices x1 and x2, return the multiplication of them\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.zeros((x1.shape[0], x2.shape[1]))\n",
    "    x1_non_zero_indices = x1.nonzero()\n",
    "    if x1.shape[0] == 1 and len(x1_non_zero_indices[1]) == 1:\n",
    "        result = x2[x1_non_zero_indices[1], :]\n",
    "    elif x1.shape[1] == 1 and len(x1_non_zero_indices[0]) == 1:\n",
    "        result[x1_non_zero_indices[0], :] = x2 * x1[x1_non_zero_indices[0], 0]\n",
    "    else:\n",
    "        result = np.matmul(x1, x2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Graded Cell\n",
    "# -----------\n",
    "\n",
    "def get_value(s, weights):\n",
    "    \"\"\"\n",
    "    Compute value of input s given the weights of a neural network\n",
    "    \"\"\"\n",
    "    ### Compute the ouput of the neural network, v, for input s\n",
    "    \n",
    "    # ----------------\n",
    "    # your code here\n",
    "    psi = my_matmul(s, weights[0][\"W\"]) + weights[0][\"b\"]\n",
    "    x = np.maximum(0, psi)\n",
    "    v = my_matmul(x, weights[1][\"W\"])+weights[1][\"b\"]\n",
    "    # ----------------\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "# Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 10 \n",
    "num_hidden_layer = 1\n",
    "s = np.array([[0, 0, 0, 1, 0]])\n",
    "\n",
    "weights_data = np.load(\"asserts/get_value_weights.npz\")\n",
    "weights = [dict() for i in range(num_hidden_layer+1)]\n",
    "weights[0][\"W\"] = weights_data[\"W0\"]\n",
    "weights[0][\"b\"] = weights_data[\"b0\"]\n",
    "weights[1][\"W\"] = weights_data[\"W1\"]\n",
    "weights[1][\"b\"] = weights_data[\"b1\"]\n",
    "\n",
    "estimated_value = get_value(s, weights)\n",
    "print (\"Estimated value: {}\".format(estimated_value))\n",
    "\n",
    "assert(np.allclose(estimated_value, [[-0.21915705]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Graded Cell\n",
    "# -----------\n",
    "\n",
    "def get_gradient(s, weights):\n",
    "    \"\"\"\n",
    "    Given inputs s and weights, return the gradient of v with respect to the weights\n",
    "    \"\"\"\n",
    "\n",
    "    ### Compute the gradient of the value function with respect to W0, b0, W1, b1 for input s\n",
    "    # grads[0][\"W\"] = ?\n",
    "    # grads[0][\"b\"] = ?\n",
    "    # grads[1][\"W\"] = ?\n",
    "    # grads[1][\"b\"] = ?\n",
    "    # Note that grads[0][\"W\"], grads[0][\"b\"], grads[1][\"W\"], and grads[1][\"b\"] should have the same shape as \n",
    "    # weights[0][\"W\"], weights[0][\"b\"], weights[1][\"W\"], and weights[1][\"b\"] respectively\n",
    "    # Note that to compute the gradients, you need to compute the activation of the hidden layer (x)\n",
    "\n",
    "    grads = [dict() for i in range(len(weights))]\n",
    "\n",
    "    # ----------------\n",
    "    # your code here\n",
    "    psi = my_matmul(s, weights[0][\"W\"]) + weights[0][\"b\"]\n",
    "    x = np.maximum(0, psi)\n",
    "    I = (x > 0)*1\n",
    "    grads[0][\"W\"] = my_matmul(s.T, weights[1][\"W\"].T * I)\n",
    "    grads[0][\"b\"] = weights[1][\"W\"].T*I\n",
    "    grads[1][\"W\"] = x.T\n",
    "    grads[1][\"b\"] = 1\n",
    "    # ----------------\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "# Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 \n",
    "num_hidden_layer = 1\n",
    "s = np.array([[0, 0, 0, 1, 0]])\n",
    "\n",
    "weights_data = np.load(\"asserts/get_gradient_weights.npz\")\n",
    "weights = [dict() for i in range(num_hidden_layer+1)]\n",
    "weights[0][\"W\"] = weights_data[\"W0\"]\n",
    "weights[0][\"b\"] = weights_data[\"b0\"]\n",
    "weights[1][\"W\"] = weights_data[\"W1\"]\n",
    "weights[1][\"b\"] = weights_data[\"b1\"]\n",
    "\n",
    "grads = get_gradient(s, weights)\n",
    "\n",
    "grads_answer = np.load(\"asserts/get_gradient_grads.npz\")\n",
    "\n",
    "assert(np.allclose(grads[0][\"W\"], grads_answer[\"W0\"]))\n",
    "assert(np.allclose(grads[0][\"b\"], grads_answer[\"b0\"]))\n",
    "assert(np.allclose(grads[1][\"W\"], grads_answer[\"W1\"]))\n",
    "assert(np.allclose(grads[1][\"b\"], grads_answer[\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grads[0][\"W\"])\n",
    "print(grads[0][\"b\"])\n",
    "print(grads[1][\"W\"])\n",
    "print(grads[1][\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Graded Cell\n",
    "# -----------\n",
    "\n",
    "class SGD(BaseOptimizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def optimizer_init(self, optimizer_info):\n",
    "        \"\"\"Setup for the optimizer.\n",
    "\n",
    "        Set parameters needed to setup the stochastic gradient descent method.\n",
    "\n",
    "        Assume optimizer_info dict contains:\n",
    "        {\n",
    "            step_size: float\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "    \n",
    "    def update_weights(self, weights, g):\n",
    "        \"\"\"\n",
    "        Given weights and update g, return updated weights\n",
    "        \"\"\"\n",
    "        for i in range(len(weights)):\n",
    "            for param in weights[i].keys():\n",
    "                \n",
    "                ### update weights\n",
    "                # weights[i][param] = None\n",
    "                \n",
    "                # ----------------\n",
    "                # your code here\n",
    "                weights[i][param] += self.step_size * g[i][param]\n",
    "                # ----------------\n",
    "                \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "# Suppose num_states = 5, num_hidden_layer = 1, and num_hidden_units = 2 \n",
    "num_hidden_layer = 1\n",
    "\n",
    "weights_data = np.load(\"asserts/update_weights_weights.npz\")\n",
    "weights = [dict() for i in range(num_hidden_layer+1)]\n",
    "weights[0][\"W\"] = weights_data[\"W0\"]\n",
    "weights[0][\"b\"] = weights_data[\"b0\"]\n",
    "weights[1][\"W\"] = weights_data[\"W1\"]\n",
    "weights[1][\"b\"] = weights_data[\"b1\"]\n",
    "\n",
    "g_data = np.load(\"asserts/update_weights_g.npz\")\n",
    "g = [dict() for i in range(num_hidden_layer+1)]\n",
    "g[0][\"W\"] = g_data[\"W0\"]\n",
    "g[0][\"b\"] = g_data[\"b0\"]\n",
    "g[1][\"W\"] = g_data[\"W1\"]\n",
    "g[1][\"b\"] = g_data[\"b1\"]\n",
    "\n",
    "test_sgd = SGD()\n",
    "optimizer_info = {\"step_size\": 0.3}\n",
    "test_sgd.optimizer_init(optimizer_info)\n",
    "updated_weights = test_sgd.update_weights(weights, g)\n",
    "\n",
    "# updated weights asserts\n",
    "updated_weights_answer = np.load(\"asserts/update_weights_updated_weights.npz\")\n",
    "\n",
    "assert(np.allclose(updated_weights[0][\"W\"], updated_weights_answer[\"W0\"]))\n",
    "assert(np.allclose(updated_weights[0][\"b\"], updated_weights_answer[\"b0\"]))\n",
    "assert(np.allclose(updated_weights[1][\"W\"], updated_weights_answer[\"W1\"]))\n",
    "assert(np.allclose(updated_weights[1][\"b\"], updated_weights_answer[\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(updated_weights[0][\"W\"])\n",
    "print(updated_weights[0][\"b\"])\n",
    "print(updated_weights[1][\"W\"])\n",
    "print(updated_weights[1][\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "class Adam(BaseOptimizer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def optimizer_init(self, optimizer_info):\n",
    "        \"\"\"Setup for the optimizer.\n",
    "\n",
    "        Set parameters needed to setup the Adam algorithm.\n",
    "\n",
    "        Assume optimizer_info dict contains:\n",
    "        {\n",
    "            num_states: integer,\n",
    "            num_hidden_layer: integer,\n",
    "            num_hidden_units: integer,\n",
    "            step_size: float, \n",
    "            self.beta_m: float\n",
    "            self.beta_v: float\n",
    "            self.epsilon: float\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_states = optimizer_info.get(\"num_states\")\n",
    "        self.num_hidden_layer = optimizer_info.get(\"num_hidden_layer\")\n",
    "        self.num_hidden_units = optimizer_info.get(\"num_hidden_units\")\n",
    "\n",
    "        # Specify Adam algorithm's hyper parameters\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
    "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
    "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
    "\n",
    "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
    "\n",
    "        # Initialize Adam algorithm's m and v\n",
    "        self.m = [dict() for i in range(self.num_hidden_layer+1)]\n",
    "        self.v = [dict() for i in range(self.num_hidden_layer+1)]\n",
    "\n",
    "        for i in range(self.num_hidden_layer+1):\n",
    "\n",
    "            # Initialize self.m[i][\"W\"], self.m[i][\"b\"], self.v[i][\"W\"], self.v[i][\"b\"] to zero\n",
    "            self.m[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i+1]))\n",
    "            self.m[i][\"b\"] = np.zeros((1, self.layer_size[i+1]))\n",
    "            self.v[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i+1]))\n",
    "            self.v[i][\"b\"] = np.zeros((1, self.layer_size[i+1]))\n",
    "\n",
    "        # Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat\n",
    "        self.beta_m_product = self.beta_m\n",
    "        self.beta_v_product = self.beta_v\n",
    "\n",
    "    def update_weights(self, weights, g):\n",
    "        \"\"\"\n",
    "        Given weights and update g, return updated weights\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            for param in weights[i].keys():\n",
    "\n",
    "                ### update self.m and self.v\n",
    "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * g[i][param]\n",
    "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (g[i][param] * g[i][param])\n",
    "\n",
    "                ### compute m_hat and v_hat\n",
    "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
    "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
    "\n",
    "                ### update weights\n",
    "                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "                \n",
    "        ### update self.beta_m_product and self.beta_v_product\n",
    "        self.beta_m_product *= self.beta_m\n",
    "        self.beta_v_product *= self.beta_v\n",
    "        \n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "def one_hot(state, num_states):\n",
    "    \"\"\"\n",
    "    Given num_state and a state, return the one-hot encoding of the state\n",
    "    \"\"\"\n",
    "    # Create the one-hot encoding of state\n",
    "    # one_hot_vector is a numpy array of shape (1, num_states)\n",
    "    \n",
    "    one_hot_vector = np.zeros((1, num_states))\n",
    "    one_hot_vector[0, int((state - 1))] = 1\n",
    "    \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Graded Cell\n",
    "# -----------\n",
    "\n",
    "class TDAgent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"td_agent\"\n",
    "        pass\n",
    "\n",
    "    def agent_init(self, agent_info={}):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the semi-gradient TD with a Neural Network.\n",
    "\n",
    "        Assume agent_info dict contains:\n",
    "        {\n",
    "            num_states: integer,\n",
    "            num_hidden_layer: integer,\n",
    "            num_hidden_units: integer,\n",
    "            step_size: float, \n",
    "            discount_factor: float,\n",
    "            self.beta_m: float\n",
    "            self.beta_v: float\n",
    "            self.epsilon: float\n",
    "            seed: int\n",
    "        }\n",
    "        \"\"\"\n",
    "    \n",
    "        # Set random seed for weights initialization for each run\n",
    "        self.rand_generator = np.random.RandomState(agent_info.get(\"seed\")) \n",
    "        \n",
    "        # Set random seed for policy for each run\n",
    "        self.policy_rand_generator = np.random.RandomState(agent_info.get(\"seed\"))\n",
    "\n",
    "        # Set attributes according to agent_info\n",
    "        self.num_states = agent_info.get(\"num_states\")\n",
    "        self.num_hidden_layer = agent_info.get(\"num_hidden_layer\")\n",
    "        self.num_hidden_units = agent_info.get(\"num_hidden_units\")\n",
    "        self.discount_factor = agent_info.get(\"discount_factor\")\n",
    "\n",
    "        ### Define the neural network's structure\n",
    "        # Specify self.layer_size which shows the number of nodes in each layer\n",
    "        # self.layer_size = np.array([None, None, None])\n",
    "        # Hint: Checkout the NN diagram at the beginning of the notebook\n",
    "        \n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.layer_size = np.array([self.num_states, self.num_hidden_units, self.num_hidden_layer])\n",
    "        # ----------------\n",
    "\n",
    "        # Initialize the neural network's parameter\n",
    "        self.weights = [dict() for i in range(self.num_hidden_layer+1)]\n",
    "        for i in range(self.num_hidden_layer+1):\n",
    "\n",
    "            ### Initialize self.weights[i][\"W\"] and self.weights[i][\"b\"] using self.rand_generator.normal()\n",
    "            # Note that The parameters of self.rand_generator.normal are mean of the distribution, \n",
    "            # standard deviation of the distribution, and output shape in the form of tuple of integers.\n",
    "            # To specify output shape, use self.layer_size.\n",
    "\n",
    "            # ----------------\n",
    "            # your code here\n",
    "            input_size, output_size = self.layer_size[i], self.layer_size[i+1]\n",
    "            self.weights[i][\"W\"] = self.rand_generator.normal(0, np.sqrt(2/input_size), (input_size, output_size))\n",
    "            self.weights[i][\"b\"] = self.rand_generator.normal(0, np.sqrt(2/input_size), (1, output_size))\n",
    "            # ----------------\n",
    "        \n",
    "        # Specify the optimizer\n",
    "        self.optimizer = Adam()\n",
    "        self.optimizer.optimizer_init({\n",
    "            \"num_states\": agent_info[\"num_states\"],\n",
    "            \"num_hidden_layer\": agent_info[\"num_hidden_layer\"],\n",
    "            \"num_hidden_units\": agent_info[\"num_hidden_units\"],\n",
    "            \"step_size\": agent_info[\"step_size\"],\n",
    "            \"beta_m\": agent_info[\"beta_m\"],\n",
    "            \"beta_v\": agent_info[\"beta_v\"],\n",
    "            \"epsilon\": agent_info[\"epsilon\"],\n",
    "        })\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "    def agent_policy(self, state):\n",
    "\n",
    "        ### Set chosen_action as 0 or 1 with equal probability. \n",
    "        chosen_action = self.policy_rand_generator.choice([0,1])    \n",
    "        return chosen_action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        ### select action given state (using self.agent_policy()), and save current state and action\n",
    "        # self.last_state = ?\n",
    "        # self.last_action = ?\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        action = self.agent_policy(state)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        # ----------------\n",
    "\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### Compute TD error\n",
    "        # delta = None\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        last_state_vector = one_hot(self.last_state, self.num_states)\n",
    "        last_value = get_value(last_state_vector, self.weights)\n",
    "        \n",
    "        state_vector = one_hot(state, self.num_states)\n",
    "        value = get_value(state_vector, self.weights)\n",
    "        \n",
    "        delta = reward + self.discount_factor * value - last_value\n",
    "        # ----------------\n",
    "\n",
    "        ### Retrieve gradients\n",
    "        # grads = None\n",
    "        # retrieve the gradients using get_gradient() function that you implemented                                                      \n",
    "        # ----------------\n",
    "        # your code here\n",
    "        grads = get_gradient(last_state_vector, self.weights)\n",
    "        # ----------------\n",
    "        ### Compute g (1 line)\n",
    "        g = [dict() for i in range(self.num_hidden_layer+1)]\n",
    "        for i in range(self.num_hidden_layer+1):\n",
    "            for param in self.weights[i].keys():\n",
    "\n",
    "                # g[i][param] = None                                        \n",
    "                # ----------------\n",
    "                # your code here\n",
    "                g[i][param] = delta * grads[i][param]\n",
    "                # ----------------\n",
    "\n",
    "        ### update the weights using self.optimizer\n",
    "        # self.weights = None\n",
    "        # use Adam_algorithm that we provided to update the neural network's parameters, self.weights.\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.weights = self.optimizer.update_weights(self.weights, g)\n",
    "                                                              \n",
    "        # ----------------\n",
    "        ### update self.last_state and self.last_action\n",
    "        # use agent_policy() method to select actions with. (only in agent_step())\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        action = self.agent_policy(state)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        # ----------------\n",
    "\n",
    "        return self.last_action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "\n",
    "        ### Compute TD error\n",
    "        # delta = None\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        last_state_vector = one_hot(self.last_state, self.num_states)\n",
    "        last_value = get_value(last_state_vector, self.weights)\n",
    "\n",
    "        delta = reward - last_value\n",
    "        # ----------------\n",
    "        ### Retrieve gradients\n",
    "        # grads = None\n",
    "        # retrieve the gradients using get_gradient() function that you implemented\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        grads = get_gradient(last_state_vector, self.weights)\n",
    "        # ----------------\n",
    "\n",
    "        ### Compute g (1 line)\n",
    "        g = [dict() for i in range(self.num_hidden_layer+1)]\n",
    "        for i in range(self.num_hidden_layer+1):\n",
    "            for param in self.weights[i].keys():\n",
    "\n",
    "                # g[i][param] = None                                        \n",
    "                # ----------------\n",
    "                # your code here\n",
    "                g[i][param] = delta * grads[i][param]\n",
    "                # ----------------\n",
    "\n",
    "        ### update the weights using self.optimizer\n",
    "        # self.weights = None\n",
    "        # use Adam_algorithm that we provided to update the neural network's parameters, self.weights.\n",
    "        # ----------------\n",
    "        # your code here\n",
    "        self.weights = self.optimizer.update_weights(self.weights, g)\n",
    "        ### END CODE HERE ###\n",
    "    def agent_message(self, message):\n",
    "        if message == 'get state value':\n",
    "            state_value = np.zeros(self.num_states)\n",
    "            for state in range(1, self.num_states + 1):\n",
    "                s = one_hot(state, self.num_states)\n",
    "                state_value[state - 1] = get_value(s, self.weights)\n",
    "            return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "agent_info = {\n",
    "    \"num_states\": 5,\n",
    "    \"num_hidden_layer\": 1,\n",
    "    \"num_hidden_units\": 2,\n",
    "    \"step_size\": 0.25,\n",
    "    \"discount_factor\": 0.9,\n",
    "    \"beta_m\": 0.9,\n",
    "    \"beta_v\": 0.99,\n",
    "    \"epsilon\": 0.0001,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "test_agent = TDAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "\n",
    "print(\"layer_size: {}\".format(test_agent.layer_size))\n",
    "assert(np.allclose(test_agent.layer_size, np.array([agent_info[\"num_states\"], \n",
    "                                                    agent_info[\"num_hidden_units\"], \n",
    "                                                    1])))\n",
    "\n",
    "assert(test_agent.weights[0][\"W\"].shape == (agent_info[\"num_states\"], agent_info[\"num_hidden_units\"]))\n",
    "assert(test_agent.weights[0][\"b\"].shape == (1, agent_info[\"num_hidden_units\"]))\n",
    "assert(test_agent.weights[1][\"W\"].shape == (agent_info[\"num_hidden_units\"], 1))\n",
    "assert(test_agent.weights[1][\"b\"].shape == (1, 1))\n",
    "\n",
    "agent_weight_answer = np.load(\"asserts/agent_init_weights_1.npz\")\n",
    "assert(np.allclose(test_agent.weights[0][\"W\"], agent_weight_answer[\"W0\"]))\n",
    "assert(np.allclose(test_agent.weights[0][\"b\"], agent_weight_answer[\"b0\"]))\n",
    "assert(np.allclose(test_agent.weights[1][\"W\"], agent_weight_answer[\"W1\"]))\n",
    "assert(np.allclose(test_agent.weights[1][\"b\"], agent_weight_answer[\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights[0][\"W\"].shape)\n",
    "print(weights[0][\"b\"].shape)\n",
    "print(weights[1][\"W\"].shape)\n",
    "print(weights[1][\"b\"].shape)\n",
    "print(weights[0][\"W\"])\n",
    "print(weights[0][\"b\"])\n",
    "print(weights[1][\"W\"])\n",
    "print(weights[1][\"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "agent_info = {\n",
    "    \"num_states\": 500,\n",
    "    \"num_hidden_layer\": 1,\n",
    "    \"num_hidden_units\": 100,\n",
    "    \"step_size\": 0.1,\n",
    "    \"discount_factor\": 1.0,\n",
    "    \"beta_m\": 0.9,\n",
    "    \"beta_v\": 0.99,\n",
    "    \"epsilon\": 0.0001,\n",
    "    \"seed\": 10,\n",
    "}\n",
    "\n",
    "# Suppose state = 250\n",
    "state = 250\n",
    "\n",
    "test_agent = TDAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "test_agent.agent_start(state)\n",
    "\n",
    "assert(test_agent.last_state == 250)\n",
    "assert(test_agent.last_action == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "agent_info = {\n",
    "    \"num_states\": 5,\n",
    "    \"num_hidden_layer\": 1,\n",
    "    \"num_hidden_units\": 2,\n",
    "    \"step_size\": 0.1,\n",
    "    \"discount_factor\": 1.0,\n",
    "    \"beta_m\": 0.9,\n",
    "    \"beta_v\": 0.99,\n",
    "    \"epsilon\": 0.0001,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "test_agent = TDAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "\n",
    "# load initial weights\n",
    "agent_initial_weight = np.load(\"asserts/agent_step_initial_weights.npz\")\n",
    "test_agent.weights[0][\"W\"] = agent_initial_weight[\"W0\"]\n",
    "test_agent.weights[0][\"b\"] = agent_initial_weight[\"b0\"]\n",
    "test_agent.weights[1][\"W\"] = agent_initial_weight[\"W1\"]\n",
    "test_agent.weights[1][\"b\"] = agent_initial_weight[\"b1\"]\n",
    "\n",
    "# load m and v for the optimizer\n",
    "m_data = np.load(\"asserts/agent_step_initial_m.npz\")\n",
    "test_agent.optimizer.m[0][\"W\"] = m_data[\"W0\"]\n",
    "test_agent.optimizer.m[0][\"b\"] = m_data[\"b0\"]\n",
    "test_agent.optimizer.m[1][\"W\"] = m_data[\"W1\"]\n",
    "test_agent.optimizer.m[1][\"b\"] = m_data[\"b1\"]\n",
    "\n",
    "v_data = np.load(\"asserts/agent_step_initial_v.npz\")\n",
    "test_agent.optimizer.v[0][\"W\"] = v_data[\"W0\"]\n",
    "test_agent.optimizer.v[0][\"b\"] = v_data[\"b0\"]\n",
    "test_agent.optimizer.v[1][\"W\"] = v_data[\"W1\"]\n",
    "test_agent.optimizer.v[1][\"b\"] = v_data[\"b1\"]\n",
    "\n",
    "# Assume the agent started at State 3\n",
    "start_state = 3\n",
    "test_agent.agent_start(start_state)\n",
    "\n",
    "# Assume the reward was 10.0 and the next state observed was State 1\n",
    "reward = 10.0\n",
    "next_state = 1\n",
    "test_agent.agent_step(reward, next_state)\n",
    "\n",
    "agent_updated_weight_answer = np.load(\"asserts/agent_step_updated_weights.npz\")\n",
    "assert(np.allclose(test_agent.weights[0][\"W\"], agent_updated_weight_answer[\"W0\"]))\n",
    "assert(np.allclose(test_agent.weights[0][\"b\"], agent_updated_weight_answer[\"b0\"]))\n",
    "assert(np.allclose(test_agent.weights[1][\"W\"], agent_updated_weight_answer[\"W1\"]))\n",
    "assert(np.allclose(test_agent.weights[1][\"b\"], agent_updated_weight_answer[\"b1\"]))\n",
    "\n",
    "assert(test_agent.last_state == 1)\n",
    "assert(test_agent.last_action == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Tested Cell\n",
    "# -----------\n",
    "# The contents of the cell will be tested by the autograder.\n",
    "# If they do not pass here, they will not pass there.\n",
    "\n",
    "agent_info = {\n",
    "    \"num_states\": 5,\n",
    "    \"num_hidden_layer\": 1,\n",
    "    \"num_hidden_units\": 2,\n",
    "    \"step_size\": 0.1,\n",
    "    \"discount_factor\": 1.0,\n",
    "    \"beta_m\": 0.9,\n",
    "    \"beta_v\": 0.99,\n",
    "    \"epsilon\": 0.0001,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "test_agent = TDAgent()\n",
    "test_agent.agent_init(agent_info)\n",
    "\n",
    "# load initial weights\n",
    "agent_initial_weight = np.load(\"asserts/agent_end_initial_weights.npz\")\n",
    "test_agent.weights[0][\"W\"] = agent_initial_weight[\"W0\"]\n",
    "test_agent.weights[0][\"b\"] = agent_initial_weight[\"b0\"]\n",
    "test_agent.weights[1][\"W\"] = agent_initial_weight[\"W1\"]\n",
    "test_agent.weights[1][\"b\"] = agent_initial_weight[\"b1\"]\n",
    "\n",
    "# load m and v for the optimizer\n",
    "m_data = np.load(\"asserts/agent_step_initial_m.npz\")\n",
    "test_agent.optimizer.m[0][\"W\"] = m_data[\"W0\"]\n",
    "test_agent.optimizer.m[0][\"b\"] = m_data[\"b0\"]\n",
    "test_agent.optimizer.m[1][\"W\"] = m_data[\"W1\"]\n",
    "test_agent.optimizer.m[1][\"b\"] = m_data[\"b1\"]\n",
    "\n",
    "v_data = np.load(\"asserts/agent_step_initial_v.npz\")\n",
    "test_agent.optimizer.v[0][\"W\"] = v_data[\"W0\"]\n",
    "test_agent.optimizer.v[0][\"b\"] = v_data[\"b0\"]\n",
    "test_agent.optimizer.v[1][\"W\"] = v_data[\"W1\"]\n",
    "test_agent.optimizer.v[1][\"b\"] = v_data[\"b1\"]\n",
    "\n",
    "# Assume the agent started at State 4\n",
    "start_state = 4\n",
    "test_agent.agent_start(start_state)\n",
    "\n",
    "# Assume the reward was 10.0 and reached the terminal state\n",
    "reward = 10.0\n",
    "test_agent.agent_end(reward)\n",
    "\n",
    "# updated weights asserts\n",
    "agent_updated_weight_answer = np.load(\"asserts/agent_end_updated_weights.npz\")\n",
    "assert(np.allclose(test_agent.weights[0][\"W\"], agent_updated_weight_answer[\"W0\"]))\n",
    "assert(np.allclose(test_agent.weights[0][\"b\"], agent_updated_weight_answer[\"b0\"]))\n",
    "assert(np.allclose(test_agent.weights[1][\"W\"], agent_updated_weight_answer[\"W1\"]))\n",
    "assert(np.allclose(test_agent.weights[1][\"b\"], agent_updated_weight_answer[\"b1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "\n",
    "true_state_val = np.load('data/true_V.npy')    \n",
    "state_distribution = np.load('data/state_distribution.npy')\n",
    "\n",
    "def calc_RMSVE(learned_state_val):\n",
    "    assert(len(true_state_val) == len(learned_state_val) == len(state_distribution))\n",
    "    MSVE = np.sum(np.multiply(state_distribution, np.square(true_state_val - learned_state_val)))\n",
    "    RMSVE = np.sqrt(MSVE)\n",
    "    return RMSVE\n",
    "\n",
    "# Define function to run experiment\n",
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save rmsve at the end of each episode\n",
    "    agent_rmsve = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                            int(experiment_parameters[\"num_episodes\"]/experiment_parameters[\"episode_eval_frequency\"]) + 1))\n",
    "    \n",
    "    # save learned state value at the end of each run\n",
    "    agent_state_val = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                environment_parameters[\"num_states\"]))\n",
    "\n",
    "    env_info = {\"num_states\": environment_parameters[\"num_states\"],\n",
    "                \"start_state\": environment_parameters[\"start_state\"],\n",
    "                \"left_terminal_state\": environment_parameters[\"left_terminal_state\"],\n",
    "                \"right_terminal_state\": environment_parameters[\"right_terminal_state\"]}\n",
    "\n",
    "    agent_info = {\"num_states\": environment_parameters[\"num_states\"],\n",
    "                  \"num_hidden_layer\": agent_parameters[\"num_hidden_layer\"],\n",
    "                  \"num_hidden_units\": agent_parameters[\"num_hidden_units\"],\n",
    "                  \"step_size\": agent_parameters[\"step_size\"],\n",
    "                  \"discount_factor\": environment_parameters[\"discount_factor\"],\n",
    "                  \"beta_m\": agent_parameters[\"beta_m\"],\n",
    "                  \"beta_v\": agent_parameters[\"beta_v\"],\n",
    "                  \"epsilon\": agent_parameters[\"epsilon\"]\n",
    "                 }\n",
    "    \n",
    "    print('Setting - Neural Network with 100 hidden units')\n",
    "    os.system('sleep 1')\n",
    "\n",
    "    # one agent setting\n",
    "    for run in tqdm(range(1, experiment_parameters[\"num_runs\"]+1)):\n",
    "        env_info[\"seed\"] = run\n",
    "        agent_info[\"seed\"] = run\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        # Compute initial RMSVE before training\n",
    "        current_V = rl_glue.rl_agent_message(\"get state value\")\n",
    "        agent_rmsve[run-1, 0] = calc_RMSVE(current_V)\n",
    "        \n",
    "        for episode in range(1, experiment_parameters[\"num_episodes\"]+1):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(0) # no step limit\n",
    "\n",
    "            if episode % experiment_parameters[\"episode_eval_frequency\"] == 0:\n",
    "                current_V = rl_glue.rl_agent_message(\"get state value\")\n",
    "                agent_rmsve[run-1, int(episode/experiment_parameters[\"episode_eval_frequency\"])] = calc_RMSVE(current_V)\n",
    "            elif episode == experiment_parameters[\"num_episodes\"]: # if last episode\n",
    "                current_V = rl_glue.rl_agent_message(\"get state value\")\n",
    "\n",
    "        agent_state_val[run-1, :] = current_V\n",
    "\n",
    "    save_name = \"{}\".format(rl_glue.agent.name).replace('.','')\n",
    "    \n",
    "    if not os.path.exists('results'):\n",
    "                os.makedirs('results')\n",
    "    \n",
    "    # save avg. state value\n",
    "    np.save(\"results/V_{}\".format(save_name), agent_state_val)\n",
    "\n",
    "    # save avg. rmsve\n",
    "    np.savez(\"results/RMSVE_{}\".format(save_name), rmsve = agent_rmsve,\n",
    "                                                   eval_freq = experiment_parameters[\"episode_eval_frequency\"],\n",
    "                                                   num_episodes = experiment_parameters[\"num_episodes\"])\n",
    "\n",
    "\n",
    "# Run Experiment\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 20,\n",
    "    \"num_episodes\" : 1000,\n",
    "    \"episode_eval_frequency\" : 10 # evaluate every 10 episode\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {\n",
    "    \"num_states\" : 500,\n",
    "    \"start_state\" : 250,\n",
    "    \"left_terminal_state\" : 0,\n",
    "    \"right_terminal_state\" : 501,\n",
    "    \"discount_factor\" : 1.0\n",
    "}\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    \"num_hidden_layer\": 1,\n",
    "    \"num_hidden_units\": 100,\n",
    "    \"step_size\": 0.001,\n",
    "    \"beta_m\": 0.9,\n",
    "    \"beta_v\": 0.999,\n",
    "    \"epsilon\": 0.0001,\n",
    "}\n",
    "\n",
    "current_env = RandomWalkEnvironment\n",
    "current_agent = TDAgent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "\n",
    "# plot result\n",
    "plot_script.plot_result([\"td_agent\"])\n",
    "\n",
    "shutil.make_archive('results', 'zip', 'results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
